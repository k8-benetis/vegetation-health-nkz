"""
FastAPI main application for Vegetation Prime module.
"""

import logging
from typing import List, Optional, Dict, Any
from datetime import date, datetime
from uuid import UUID
from contextlib import asynccontextmanager

from fastapi import FastAPI, Depends, HTTPException, status, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session

from app.database import get_db_with_tenant, get_db_session, init_db
from app.middleware.auth import require_auth, get_tenant_id

# Helper function for database dependency with tenant context
def get_db_for_tenant(current_user: dict = Depends(require_auth)):
    """Get database session with tenant context.
    
    This is a FastAPI dependency that depends on current_user.
    Returns a generator that FastAPI will handle automatically.
    """
    # Call get_db_with_tenant and yield from it
    for db in get_db_with_tenant(current_user['tenant_id']):
        yield db
from app.middleware.service_auth import require_service_auth
from app.models import (
    VegetationConfig, VegetationJob, VegetationScene,
    VegetationIndexCache, VegetationCustomFormula,
    VegetationPlanLimits, VegetationUsageStats
)
from app.tasks import download_sentinel2_scene, calculate_vegetation_index
from app.services.storage import create_storage_service
from app.services.fiware_integration import FIWAREMapper, FIWAREClient
from app.services.limits import LimitsValidator
from app.services.usage_tracker import UsageTracker
from app.middleware.limits import validate_limits_dependency
from decimal import Decimal
from app.api.tiles import router as tiles_router

logger = logging.getLogger(__name__)

# Configure logging
logging.basicConfig(level=logging.INFO)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan events."""
    # Startup
    logger.info("Starting Vegetation Prime API...")
    # init_db()  # Uncomment if you want to auto-create tables
    yield
    # Shutdown
    logger.info("Shutting down Vegetation Prime API...")


# Create FastAPI app
app = FastAPI(
    title="Vegetation Prime API",
    description="High-performance vegetation intelligence suite for Nekazari Platform",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include tile router
app.include_router(tiles_router)


# =============================================================================
# Pydantic Models
# =============================================================================

class JobCreateRequest(BaseModel):
    """Request model for creating a job."""
    job_type: str = Field(..., description="Type of job: download, process, calculate_index")
    entity_id: Optional[str] = Field(None, description="FIWARE entity ID (AgriParcel)")
    entity_type: str = Field("AgriParcel", description="Entity type")
    bounds: Optional[Dict[str, Any]] = Field(None, description="GeoJSON polygon bounds")
    start_date: Optional[date] = Field(None, description="Start date for scene search")
    end_date: Optional[date] = Field(None, description="End date for scene search")
    parameters: Dict[str, Any] = Field(default_factory=dict, description="Additional parameters")


class JobResponse(BaseModel):
    """Response model for job."""
    id: str
    tenant_id: str
    job_type: str
    status: str
    progress_percentage: int
    progress_message: Optional[str]
    created_at: datetime
    started_at: Optional[datetime]
    completed_at: Optional[datetime]
    result: Optional[Dict[str, Any]]
    error_message: Optional[str]


class ConfigUpdateRequest(BaseModel):
    """Request model for updating configuration."""
    copernicus_client_id: Optional[str] = None
    copernicus_client_secret: Optional[str] = None  # Will be encrypted
    default_index_type: Optional[str] = None
    cloud_coverage_threshold: Optional[float] = None
    auto_process: Optional[bool] = None
    storage_type: Optional[str] = None
    # NOTE: storage_bucket removed - now auto-generated from tenant_id for security
    # NOTE: Copernicus credentials are now managed by platform, but module-specific
    # credentials can still be set as fallback for backward compatibility


class IndexCalculationRequest(BaseModel):
    """Request model for calculating index.
    
    Supports two modes:
    1. Single scene: Provide scene_id
    2. Temporal composite: Provide start_date and end_date (cloud-free composite)
    """
    scene_id: Optional[str] = Field(None, description="Scene ID for single scene calculation")
    index_type: str = Field(..., description="NDVI, EVI, SAVI, GNDVI, NDRE, CUSTOM")
    formula: Optional[str] = Field(None, description="Custom formula if index_type is CUSTOM")
    entity_id: Optional[str] = None
    # Temporal composite options
    start_date: Optional[date] = Field(None, description="Start date for temporal composite (cloud-free)")
    end_date: Optional[date] = Field(None, description="End date for temporal composite (cloud-free)")


class TimeseriesRequest(BaseModel):
    """Request model for timeseries."""
    entity_id: str
    index_type: str
    start_date: Optional[date] = None
    end_date: Optional[date] = None


# =============================================================================
# Endpoints
# =============================================================================

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "service": "vegetation-prime"}


@app.get("/api/vegetation/health")
async def vegetation_health_check():
    """Health check endpoint for ingress routing."""
    return {"status": "healthy", "service": "vegetation-prime"}


class LimitsSyncRequest(BaseModel):
    """Request model for syncing limits from Core."""
    tenant_id: str
    plan_type: str
    plan_name: Optional[str] = None
    monthly_ha_limit: float
    daily_ha_limit: float
    daily_jobs_limit: int
    monthly_jobs_limit: int
    daily_download_jobs_limit: int
    daily_process_jobs_limit: int
    daily_calculate_jobs_limit: int
    is_active: bool = True
    synced_by: Optional[str] = None


class UsageResponse(BaseModel):
    """Response model for usage statistics (simplified format)."""
    plan: str
    volume: Dict[str, float]
    frequency: Dict[str, int]
    _detailed: Optional[Dict[str, Any]] = None  # Internal detailed info


@app.post("/api/vegetation/jobs", response_model=JobResponse, status_code=status.HTTP_201_CREATED)
async def create_job(
    request: JobCreateRequest,
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Create a new vegetation processing job.
    
    Validates limits before creating the job.
    """
    try:
        # Validate limits BEFORE creating job
        validator = LimitsValidator(db, current_user['tenant_id'])
        
        ha_to_process = None
        if request.ha_to_process:
            from decimal import Decimal
            ha_to_process = Decimal(str(request.ha_to_process))
        
        is_allowed, error_message, usage_info = validator.check_all_limits(
            job_type=request.job_type,
            bounds=request.bounds,
            ha_to_process=ha_to_process
        )
        
        if not is_allowed:
            raise HTTPException(
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                detail={
                    'error': 'Limit exceeded',
                    'message': error_message,
                    'usage': usage_info,
                    'limits': validator.limits,
                }
            )
        
        # Limits OK, create job
        job = VegetationJob(
            tenant_id=current_user['tenant_id'],
            job_type=request.job_type,
            entity_id=request.entity_id,
            entity_type=request.entity_type,
            start_date=request.start_date,
            end_date=request.end_date,
            parameters=request.parameters,
            created_by=current_user.get('user_id')
        )
        
        db.add(job)
        db.commit()
        db.refresh(job)
        
        # Record usage (increment counters)
        from decimal import Decimal
        ha_processed = ha_to_process or UsageTracker.calculate_area_hectares(request.bounds)
        UsageTracker.record_job_usage(
            db=db,
            tenant_id=current_user['tenant_id'],
            job_id=str(job.id),
            job_type=request.job_type,
            bounds=request.bounds,
            ha_processed=ha_processed
        )
        
        # Queue Celery task based on job type
        if request.job_type == 'download':
            download_sentinel2_scene.delay(
                str(job.id),
                current_user['tenant_id'],
                request.parameters
            )
        elif request.job_type == 'calculate_index':
            calculate_vegetation_index.delay(
                str(job.id),
                current_user['tenant_id'],
                request.parameters.get('scene_id'),
                request.parameters.get('index_type'),
                request.parameters.get('formula')
            )
        
        return JobResponse(
            id=str(job.id),
            tenant_id=job.tenant_id,
            job_type=job.job_type,
            status=job.status,
            progress_percentage=job.progress_percentage,
            progress_message=job.progress_message,
            created_at=job.created_at,
            started_at=job.started_at,
            completed_at=job.completed_at,
            result=job.result,
            error_message=job.error_message
        )
        
    except Exception as e:
        logger.error(f"Error creating job: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create job: {str(e)}"
        )


@app.get("/api/vegetation/jobs/{job_id}", response_model=JobResponse)
async def get_job(
    job_id: UUID,
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Get job status and details."""
    job = db.query(VegetationJob).filter(
        VegetationJob.id == job_id,
        VegetationJob.tenant_id == current_user['tenant_id']
    ).first()
    
    if not job:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Job not found"
        )
    
    return JobResponse(
        id=str(job.id),
        tenant_id=job.tenant_id,
        job_type=job.job_type,
        status=job.status,
        progress_percentage=job.progress_percentage,
        progress_message=job.progress_message,
        created_at=job.created_at,
        started_at=job.started_at,
        completed_at=job.completed_at,
        result=job.result,
        error_message=job.error_message
    )


class JobDetailsResponse(BaseModel):
    """Response model for job details with statistics."""
    job: JobResponse
    index_stats: Optional[Dict[str, Any]] = None
    timeseries: Optional[List[Dict[str, Any]]] = None
    scene_info: Optional[Dict[str, Any]] = None


@app.get("/api/vegetation/jobs/{job_id}/details", response_model=JobDetailsResponse)
async def get_job_details(
    job_id: UUID,
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Get detailed job information including statistics and timeseries."""
    from app.models import VegetationIndexCache, VegetationScene
    
    # Get job
    job = db.query(VegetationJob).filter(
        VegetationJob.id == job_id,
        VegetationJob.tenant_id == current_user['tenant_id']
    ).first()
    
    if not job:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Job not found"
        )
    
    job_response = JobResponse(
        id=str(job.id),
        tenant_id=job.tenant_id,
        job_type=job.job_type,
        status=job.status,
        progress_percentage=job.progress_percentage,
        progress_message=job.progress_message,
        created_at=job.created_at,
        started_at=job.started_at,
        completed_at=job.completed_at,
        result=job.result,
        error_message=job.error_message
    )
    
    # Get index statistics if job is completed and has result
    index_stats = None
    scene_info = None
    
    if job.status == 'completed' and job.result:
        result = job.result if isinstance(job.result, dict) else {}
        
        # Extract statistics from result
        if 'statistics' in result:
            stats = result['statistics']
            index_stats = {
                'mean': float(stats.get('mean', 0)) if stats.get('mean') is not None else None,
                'min': float(stats.get('min', 0)) if stats.get('min') is not None else None,
                'max': float(stats.get('max', 0)) if stats.get('max') is not None else None,
                'std_dev': float(stats.get('std', 0)) if stats.get('std') is not None else None,
                'pixel_count': stats.get('pixel_count', 0) if stats.get('pixel_count') is not None else None,
            }
        
        # Get scene info if available
        if 'scene_id' in result:
            scene_id = result['scene_id']
            scene = db.query(VegetationScene).filter(
                VegetationScene.id == scene_id,
                VegetationScene.tenant_id == current_user['tenant_id']
            ).first()
            
            if scene:
                scene_info = {
                    'id': str(scene.id),
                    'sensing_date': scene.sensing_date.isoformat() if scene.sensing_date else None,
                    'cloud_coverage': float(scene.cloud_coverage) if scene.cloud_coverage else None,
                    'scene_id': scene.scene_id,
                }
    
    # Get timeseries if job has entity_id
    timeseries = None
    if job.entity_id:
        try:
            # Get all index calculations for this entity
            indices = db.query(VegetationIndexCache).filter(
                VegetationIndexCache.entity_id == job.entity_id,
                VegetationIndexCache.tenant_id == current_user['tenant_id']
            ).order_by(VegetationIndexCache.calculated_at.desc()).limit(50).all()
            
            if indices:
                timeseries = []
                for idx in indices:
                    timeseries.append({
                        'date': idx.calculated_at,
                        'index_type': idx.index_type,
                        'mean_value': float(idx.mean_value) if idx.mean_value is not None else None,
                        'min_value': float(idx.min_value) if idx.min_value is not None else None,
                        'max_value': float(idx.max_value) if idx.max_value is not None else None,
                        'std_dev': float(idx.std_dev) if idx.std_dev is not None else None,
                    })
        except Exception as e:
            logger.warning(f"Error fetching timeseries: {e}")
    
    return JobDetailsResponse(
        job=job_response,
        index_stats=index_stats,
        timeseries=timeseries,
        scene_info=scene_info
    )


@app.get("/api/vegetation/jobs/{job_id}/histogram")
async def get_job_histogram(
    job_id: UUID,
    bins: int = 50,
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Get histogram distribution for a completed job.
    
    Returns approximate distribution based on statistics (mean, std_dev) 
    using a normal distribution approximation.
    For exact distribution, would need to read the full raster (expensive).
    """
    from app.models import VegetationIndexCache
    
    # Get job
    job = db.query(VegetationJob).filter(
        VegetationJob.id == job_id,
        VegetationJob.tenant_id == current_user['tenant_id']
    ).first()
    
    if not job:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Job not found"
        )
    
    if job.status != 'completed' or not job.result:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Job is not completed or has no results"
        )
    
    result = job.result if isinstance(job.result, dict) else {}
    stats = result.get('statistics', {})
    
    if not stats:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No statistics available for this job"
        )
    
    mean = float(stats.get('mean', 0)) if stats.get('mean') is not None else 0
    std_dev = float(stats.get('std', 0)) if stats.get('std') is not None else 0
    min_val = float(stats.get('min', -1)) if stats.get('min') is not None else -1
    max_val = float(stats.get('max', 1)) if stats.get('max') is not None else 1
    pixel_count = int(stats.get('pixel_count', 0)) if stats.get('pixel_count') is not None else 0
    
    # Try to get exact distribution from VegetationIndexCache if available
    index_cache = None
    if job.job_type == 'calculate_index' and 'scene_id' in result:
        scene_id = result['scene_id']
        index_type = result.get('index_type', 'NDVI')
        
        # Find the cached index
        index_cache = db.query(VegetationIndexCache).filter(
            VegetationIndexCache.scene_id == scene_id,
            VegetationIndexCache.index_type == index_type,
            VegetationIndexCache.tenant_id == current_user['tenant_id']
        ).first()
    
    # Generate histogram bins
    try:
        import numpy as np
    except ImportError:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="numpy is required for histogram calculation"
        )
    import math
    
    # Use actual min/max from statistics
    bin_edges = np.linspace(min_val, max_val, bins + 1)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    
    # Generate approximate distribution using normal distribution
    # This is an approximation - for exact distribution, would need to read raster
    if std_dev > 0 and pixel_count > 0:
        # Normal distribution CDF approximation (using error function)
        def normal_cdf(x, mu, sigma):
            """Cumulative distribution function for normal distribution."""
            if sigma == 0:
                return 1.0 if x >= mu else 0.0
            z = (x - mu) / sigma
            return 0.5 * (1 + math.erf(z / math.sqrt(2)))
        
        # Calculate probabilities for each bin
        bin_probs = []
        for i in range(len(bin_edges) - 1):
            prob = normal_cdf(bin_edges[i + 1], mean, std_dev) - normal_cdf(bin_edges[i], mean, std_dev)
            bin_probs.append(max(0, prob))
        
        # Normalize probabilities
        total_prob = sum(bin_probs)
        if total_prob > 0:
            bin_probs = [p / total_prob for p in bin_probs]
        
        # Convert to counts
        bin_counts = np.array([int(p * pixel_count) for p in bin_probs])
    else:
        # If no std_dev, create uniform distribution
        bin_counts = np.full(bins, pixel_count // bins)
    
    # Ensure total matches pixel_count
    total = int(bin_counts.sum())
    if total < pixel_count:
        diff = pixel_count - total
        # Distribute remainder to bins with most counts
        bin_counts = bin_counts.astype(int)
        indices = np.argsort(bin_counts)[::-1]
        for i in range(min(diff, len(indices))):
            bin_counts[indices[i]] += 1
    elif total > pixel_count:
        # Remove excess from bins with most counts
        diff = total - pixel_count
        indices = np.argsort(bin_counts)[::-1]
        for i in range(min(diff, len(indices))):
            if bin_counts[indices[i]] > 0:
                bin_counts[indices[i]] -= 1
    
    return {
        "bins": bin_centers.tolist(),
        "counts": bin_counts.tolist(),
        "statistics": {
            "mean": mean,
            "min": min_val,
            "max": max_val,
            "std_dev": std_dev,
            "pixel_count": pixel_count
        },
        "approximation": True,  # Indicates this is an approximation
        "note": "Distribution approximated from statistics. For exact distribution, raster reading would be required."
    }


@app.get("/api/vegetation/jobs")
async def list_jobs(
    status_filter: Optional[str] = None,
    limit: int = 50,
    offset: int = 0,
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """List jobs for current tenant."""
    query = db.query(VegetationJob).filter(
        VegetationJob.tenant_id == current_user['tenant_id']
    )
    
    if status_filter:
        query = query.filter(VegetationJob.status == status_filter)
    
    jobs = query.order_by(VegetationJob.created_at.desc()).offset(offset).limit(limit).all()
    
    return {
        "jobs": [
            {
                "id": str(job.id),
                "job_type": job.job_type,
                "status": job.status,
                "progress_percentage": job.progress_percentage,
                "created_at": job.created_at.isoformat(),
                "completed_at": job.completed_at.isoformat() if job.completed_at else None
            }
            for job in jobs
        ],
        "total": query.count()
    }


@app.get("/api/vegetation/indices")
async def get_indices(
    entity_id: Optional[str] = None,
    scene_id: Optional[str] = None,
    index_type: Optional[str] = None,
    format: str = "geojson",  # geojson or xyz
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Get vegetation indices (as tiles or GeoJSON)."""
    query = db.query(VegetationIndexCache).filter(
        VegetationIndexCache.tenant_id == current_user['tenant_id']
    )
    
    if entity_id:
        query = query.filter(VegetationIndexCache.entity_id == entity_id)
    if scene_id:
        query = query.filter(VegetationIndexCache.scene_id == UUID(scene_id))
    if index_type:
        query = query.filter(VegetationIndexCache.index_type == index_type)
    
    indices = query.all()
    
    if format == "geojson":
        # Return as GeoJSON
        return {
            "type": "FeatureCollection",
            "features": [
                {
                    "type": "Feature",
                    "properties": {
                        "index_type": idx.index_type,
                        "mean_value": float(idx.mean_value) if idx.mean_value else None,
                        "min_value": float(idx.min_value) if idx.min_value else None,
                        "max_value": float(idx.max_value) if idx.max_value else None,
                        "calculated_at": idx.calculated_at
                    },
                    "geometry": idx.statistics_geojson if idx.statistics_geojson else None
                }
                for idx in indices
            ]
        }
    else:
        # Return tile URLs
        return {
            "tiles": [
                {
                    "index_type": idx.index_type,
                    "tiles_url": idx.result_tiles_path,
                    "calculated_at": idx.calculated_at
                }
                for idx in indices
            ]
        }


@app.get("/api/vegetation/scenes")
async def list_scenes(
    entity_id: Optional[str] = None,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    limit: int = 50,
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """List available scenes for current tenant.
    
    Args:
        entity_id: Optional entity ID filter
        start_date: Optional start date filter
        end_date: Optional end date filter
        limit: Maximum number of results
        
    Returns:
        List of scenes with metadata
    """
    query = db.query(VegetationScene).filter(
        VegetationScene.tenant_id == current_user['tenant_id']
    )
    
    if entity_id:
        # Join with indices to filter by entity
        query = query.join(VegetationIndexCache).filter(
            VegetationIndexCache.entity_id == entity_id
        )
    
    if start_date:
        query = query.filter(VegetationScene.sensing_date >= start_date)
    if end_date:
        query = query.filter(VegetationScene.sensing_date <= end_date)
    
    scenes = query.order_by(VegetationScene.sensing_date.desc()).limit(limit).all()
    
    return {
        "scenes": [
            {
                "id": str(scene.id),
                "scene_id": scene.scene_id,
                "sensing_date": scene.sensing_date.isoformat(),
                "cloud_coverage": float(scene.cloud_coverage) if scene.cloud_coverage else None,
                "platform": scene.platform,
                "product_type": scene.product_type,
            }
            for scene in scenes
        ],
        "total": query.count()
    }


@app.get("/api/vegetation/timeseries")
async def get_timeseries(
    request: TimeseriesRequest = Depends(),
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Get time series data for vegetation indices."""
    query = db.query(VegetationIndexCache).filter(
        VegetationIndexCache.tenant_id == current_user['tenant_id'],
        VegetationIndexCache.entity_id == request.entity_id,
        VegetationIndexCache.index_type == request.index_type
    )
    
    if request.start_date:
        # Join with scenes to filter by date
        query = query.join(VegetationScene).filter(
            VegetationScene.sensing_date >= request.start_date
        )
    if request.end_date:
        query = query.join(VegetationScene).filter(
            VegetationScene.sensing_date <= request.end_date
        )
    
    indices = query.order_by(VegetationIndexCache.calculated_at).all()
    
    return {
        "entity_id": request.entity_id,
        "index_type": request.index_type,
        "data_points": [
            {
                "date": idx.calculated_at,
                "value": float(idx.mean_value) if idx.mean_value else None,
                "min": float(idx.min_value) if idx.min_value else None,
                "max": float(idx.max_value) if idx.max_value else None,
                "std": float(idx.std_dev) if idx.std_dev else None
            }
            for idx in indices
        ]
    }


@app.get("/api/vegetation/config/credentials-status")
async def get_credentials_status(
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Check Copernicus credentials availability status.
    
    Returns information about whether credentials are available from platform
    or module-specific configuration.
    """
    from app.services.platform_credentials import get_copernicus_credentials_with_fallback
    from app.models import VegetationConfig
    
    # Get module config for fallback check
    config = db.query(VegetationConfig).filter(
        VegetationConfig.tenant_id == current_user['tenant_id']
    ).first()
    
    # Try to get credentials
    platform_creds = None
    module_creds = None
    
    try:
        from app.services.platform_credentials import get_copernicus_credentials
        # get_copernicus_credentials now connects directly to platform database
        platform_creds = get_copernicus_credentials()
        if platform_creds:
            logger.info(f"Successfully retrieved platform credentials for tenant {current_user['tenant_id']}")
        else:
            logger.debug(f"No platform credentials found for tenant {current_user['tenant_id']}")
    except Exception as e:
        logger.warning(f"Error checking platform credentials for tenant {current_user['tenant_id']}: {e}")
    
    # Check module-specific credentials
    if config and config.copernicus_client_id and config.copernicus_client_secret_encrypted:
        module_creds = {
            'client_id': config.copernicus_client_id,
            'client_secret': config.copernicus_client_secret_encrypted
        }
    
    # Determine status
    if platform_creds:
        logger.info(f"Platform credentials found for tenant {current_user['tenant_id']}: {platform_creds.get('client_id', 'N/A')[:15]}...")
        return {
            "available": True,
            "source": "platform",
            "message": "Credenciales disponibles desde la plataforma",
            "client_id_preview": platform_creds['client_id'][:10] + "..." if len(platform_creds['client_id']) > 10 else platform_creds['client_id']
        }
    elif module_creds:
        return {
            "available": True,
            "source": "module",
            "message": "Credenciales disponibles desde configuración del módulo (fallback)",
            "client_id_preview": module_creds['client_id'][:10] + "..." if len(module_creds['client_id']) > 10 else module_creds['client_id']
        }
    else:
        logger.warning(f"[credentials-status] No credentials found for tenant {current_user['tenant_id']}")
        result = {
            "available": False,
            "source": None,
            "message": "No se encontraron credenciales. Configure las credenciales en el panel de administración de la plataforma.",
            "client_id_preview": None
        }
        logger.info(f"[credentials-status] Returning: {result}")
        return result


@app.post("/api/vegetation/config", status_code=status.HTTP_200_OK)
@app.put("/api/vegetation/config", status_code=status.HTTP_200_OK)  # Support PUT as well for compatibility
async def update_config(
    request: ConfigUpdateRequest,
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Update tenant configuration."""
    config = db.query(VegetationConfig).filter(
        VegetationConfig.tenant_id == current_user['tenant_id']
    ).first()
    
    if not config:
        config = VegetationConfig(tenant_id=current_user['tenant_id'])
        db.add(config)
    
    # Update fields
    # NOTE: Copernicus credentials are now managed by the platform (external_api_credentials table)
    # Module-specific credentials are kept as fallback only
    if request.copernicus_client_id is not None:
        config.copernicus_client_id = request.copernicus_client_id
    if request.copernicus_client_secret is not None:
        # TODO: Encrypt secret before storing
        config.copernicus_client_secret_encrypted = request.copernicus_client_secret
    if request.default_index_type is not None:
        config.default_index_type = request.default_index_type
    if request.cloud_coverage_threshold is not None:
        config.cloud_coverage_threshold = request.cloud_coverage_threshold
    if request.auto_process is not None:
        config.auto_process = request.auto_process
    if request.storage_type is not None:
        config.storage_type = request.storage_type
    # NOTE: storage_bucket is now auto-generated from tenant_id for security
    # Users cannot specify bucket names to prevent conflicts and security issues
    
    db.commit()
    db.refresh(config)
    
    return {"message": "Configuration updated", "config": config.to_dict()}


@app.get("/api/vegetation/config")
async def get_config(
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Get tenant configuration."""
    config = db.query(VegetationConfig).filter(
        VegetationConfig.tenant_id == current_user['tenant_id']
    ).first()
    
    if not config:
        # Return default config
        return {
            "tenant_id": current_user['tenant_id'],
            "default_index_type": "NDVI",
            "cloud_coverage_threshold": 20.0,
            "auto_process": True,
            "storage_type": "s3"
        }
    
    return config.to_dict()


@app.post("/api/vegetation/admin/sync-limits", status_code=status.HTTP_200_OK)
async def sync_limits(
    request: LimitsSyncRequest,
    _: None = Depends(require_service_auth),  # Service authentication required
    db: Session = Depends(get_db_session)  # Direct DB session (no tenant context needed for admin)
):
    """Sync limits from Core Platform.
    
    This endpoint is called by the Core Platform to push limit updates.
    Protected by X-Service-Auth header validation.
    """
    
    try:
        from decimal import Decimal
        
        # Get or create limits record
        limits = db.query(VegetationPlanLimits).filter(
            VegetationPlanLimits.tenant_id == request.tenant_id
        ).first()
        
        if not limits:
            limits = VegetationPlanLimits(tenant_id=request.tenant_id)
            db.add(limits)
        
        # Update limits
        limits.plan_type = request.plan_type
        limits.plan_name = request.plan_name
        limits.monthly_ha_limit = Decimal(str(request.monthly_ha_limit))
        limits.daily_ha_limit = Decimal(str(request.daily_ha_limit))
        limits.daily_jobs_limit = request.daily_jobs_limit
        limits.monthly_jobs_limit = request.monthly_jobs_limit
        limits.daily_download_jobs_limit = request.daily_download_jobs_limit
        limits.daily_process_jobs_limit = request.daily_process_jobs_limit
        limits.daily_calculate_jobs_limit = request.daily_calculate_jobs_limit
        limits.is_active = request.is_active
        limits.synced_at = datetime.utcnow()
        limits.synced_by = request.synced_by or 'core-platform'
        
        db.commit()
        db.refresh(limits)
        
        logger.info(f"Limits synced for tenant {request.tenant_id}: {request.plan_type}")
        
        return {
            "message": "Limits synced successfully",
            "tenant_id": request.tenant_id,
            "plan_type": request.plan_type,
            "limits": {
                "monthly_ha_limit": float(limits.monthly_ha_limit),
                "daily_ha_limit": float(limits.daily_ha_limit),
                "daily_jobs_limit": limits.daily_jobs_limit,
            }
        }
        
    except Exception as e:
        logger.error(f"Error syncing limits: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to sync limits: {str(e)}"
        )


@app.get("/api/vegetation/usage/current", response_model=UsageResponse)
async def get_current_usage(
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Get current usage statistics for the tenant.
    
    Special handling for PlatformAdmin users (unlimited access).
    """
    # Handle PlatformAdmin specially - unlimited access
    user_roles = current_user.get('roles', []) or []
    if 'PlatformAdmin' in user_roles or current_user.get('role') == 'PlatformAdmin':
        # Return a very high limit instead of inf (JSON doesn't support inf)
        return UsageResponse(
            plan='ADMIN',
            volume={
                'used_ha': 0.0,
                'limit_ha': 999999.0  # Effectively unlimited
            },
            frequency={
                'used_jobs_today': 0,
                'limit_jobs_today': 999999
            }
        )
    
    validator = LimitsValidator(db, current_user['tenant_id'])
    usage = validator.get_current_usage()
    
    # Improve plan name if using defaults
    from app.models import VegetationPlanLimits
    if usage.get('plan') == 'BASIC' and not validator.limits.get('plan_name'):
        # Check if limits were loaded from DB or are defaults
        limits_in_db = db.query(VegetationPlanLimits).filter(
            VegetationPlanLimits.tenant_id == current_user['tenant_id'],
            VegetationPlanLimits.is_active == True
        ).first()
        
        if not limits_in_db:
            # Using defaults - plan not configured
            usage['plan'] = 'NO_CONFIGURADO'
    
    return UsageResponse(**usage)


@app.post("/api/vegetation/calculate")
async def calculate_index(
    request: IndexCalculationRequest,
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Calculate vegetation index for a scene or temporal composite.
    
    Two modes:
    1. Single scene: Provide scene_id
    2. Temporal composite (cloud-free): Provide start_date and end_date
    
    Validates limits before creating the job.
    """
    try:
        # Validate request: must have either scene_id OR (start_date AND end_date)
        if not request.scene_id and not (request.start_date and request.end_date):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Either scene_id or both start_date and end_date must be provided"
            )
        
        if request.scene_id and (request.start_date or request.end_date):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Cannot provide both scene_id and date range. Use either single scene or temporal composite."
            )
        
        # Hard limits for temporal composite
        if request.start_date and request.end_date:
            date_range_days = (request.end_date - request.start_date).days
            if date_range_days > 90:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Date range cannot exceed 90 days. Provided: {date_range_days} days"
                )
            if date_range_days < 0:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="end_date must be after start_date"
                )
        
        # Validate limits (calculate_index jobs have minimal area, but check frequency)
        validator = LimitsValidator(db, current_user['tenant_id'])
        
        is_allowed, error_message, usage_info = validator.check_frequency_limit('calculate_index')
        
        if not is_allowed:
            raise HTTPException(
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                detail={
                    'error': 'Limit exceeded',
                    'message': error_message,
                    'usage': usage_info,
                    'limits': validator.limits,
                }
            )
        
        # Create job
        job = VegetationJob(
            tenant_id=current_user['tenant_id'],
            job_type='calculate_index',
            parameters={
                'scene_id': request.scene_id,
                'index_type': request.index_type,
                'formula': request.formula,
                'start_date': request.start_date.isoformat() if request.start_date else None,
                'end_date': request.end_date.isoformat() if request.end_date else None,
            },
            entity_id=request.entity_id,
            start_date=request.start_date,
            end_date=request.end_date,
            created_by=current_user.get('user_id')
        )
        
        db.add(job)
        db.commit()
        db.refresh(job)
        
        # Record usage (calculate jobs have minimal area, but count as job)
        UsageTracker.record_job_usage(
            db=db,
            tenant_id=current_user['tenant_id'],
            job_id=str(job.id),
            job_type='calculate_index',
            bounds=None,
            ha_processed=Decimal('0.0')  # Calculate jobs don't process new area
        )
        
        # Queue calculation task
        calculate_vegetation_index.delay(
            str(job.id),
            current_user['tenant_id'],
            request.scene_id,
            request.index_type,
            request.formula,
            request.start_date.isoformat() if request.start_date else None,
            request.end_date.isoformat() if request.end_date else None
        )
        
        return {
            "job_id": str(job.id),
            "message": "Index calculation queued"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error calculating index: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to calculate index: {str(e)}"
        )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



# =============================================================================
# Smart Timeline API - Historical Stats for Charts
# =============================================================================

class SceneStats(BaseModel):
    """Scene statistics for timeline chart."""
    scene_id: str
    sensing_date: str
    mean_value: Optional[float] = None
    min_value: Optional[float] = None
    max_value: Optional[float] = None
    std_dev: Optional[float] = None
    cloud_coverage: Optional[float] = None

class TimelineStatsResponse(BaseModel):
    """Response for timeline stats endpoint."""
    entity_id: str
    index_type: str
    stats: List[SceneStats]
    period_start: str
    period_end: str


@app.get("/scenes/{entity_id}/stats", response_model=TimelineStatsResponse)
async def get_scene_stats(
    entity_id: str,
    index_type: str = "NDVI",
    months: int = 12,
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Get historical statistics for timeline chart.
    
    Returns mean index values per scene for the specified period.
    Used by Smart Timeline to render the line chart.
    
    Args:
        entity_id: Entity ID (AgriParcel URN)
        index_type: Index type (NDVI, NDMI, SAVI, etc.)
        months: Number of months to include (default: 12)
    
    Returns:
        TimelineStatsResponse with stats per scene
    """
    from datetime import timedelta
    from sqlalchemy import and_, desc
    
    # Calculate date range
    end_date = datetime.utcnow().date()
    start_date = end_date - timedelta(days=months * 30)
    
    # Query scenes with their index stats
    results = db.query(
        VegetationScene,
        VegetationIndexCache
    ).outerjoin(
        VegetationIndexCache,
        and_(
            VegetationIndexCache.scene_id == VegetationScene.id,
            VegetationIndexCache.entity_id == entity_id,
            VegetationIndexCache.index_type == index_type,
            VegetationIndexCache.tenant_id == current_user['tenant_id']
        )
    ).filter(
        VegetationScene.tenant_id == current_user['tenant_id'],
        VegetationScene.sensing_date >= start_date,
        VegetationScene.sensing_date <= end_date,
        VegetationScene.is_valid == True
    ).order_by(
        desc(VegetationScene.sensing_date)
    ).all()
    
    stats = []
    for scene, cache in results:
        cloud_cov = None
        if scene.cloud_coverage:
            try:
                cloud_cov = float(scene.cloud_coverage)
            except:
                pass
        
        stats.append(SceneStats(
            scene_id=str(scene.id),
            sensing_date=scene.sensing_date.isoformat(),
            mean_value=float(cache.mean_value) if cache and cache.mean_value else None,
            min_value=float(cache.min_value) if cache and cache.min_value else None,
            max_value=float(cache.max_value) if cache and cache.max_value else None,
            std_dev=float(cache.std_dev) if cache and cache.std_dev else None,
            cloud_coverage=cloud_cov
        ))
    
    return TimelineStatsResponse(
        entity_id=entity_id,
        index_type=index_type,
        stats=stats,
        period_start=start_date.isoformat(),
        period_end=end_date.isoformat()
    )


@app.get("/scenes/{entity_id}/compare-years", response_model=Dict[str, Any])
async def compare_years(
    entity_id: str,
    index_type: str = "NDVI",
    current_user: dict = Depends(require_auth),
    db: Session = Depends(get_db_for_tenant)
):
    """Compare current year vs previous year.
    
    Returns two series of stats for overlay comparison in the timeline.
    """
    from datetime import timedelta
    from sqlalchemy import and_, desc, extract
    
    current_year = datetime.utcnow().year
    
    def get_year_stats(year: int):
        start_date = date(year, 1, 1)
        end_date = date(year, 12, 31)
        
        results = db.query(
            VegetationScene,
            VegetationIndexCache
        ).outerjoin(
            VegetationIndexCache,
            and_(
                VegetationIndexCache.scene_id == VegetationScene.id,
                VegetationIndexCache.entity_id == entity_id,
                VegetationIndexCache.index_type == index_type,
                VegetationIndexCache.tenant_id == current_user['tenant_id']
            )
        ).filter(
            VegetationScene.tenant_id == current_user['tenant_id'],
            VegetationScene.sensing_date >= start_date,
            VegetationScene.sensing_date <= end_date,
            VegetationScene.is_valid == True
        ).order_by(
            VegetationScene.sensing_date
        ).all()
        
        return [
            {
                "month": scene.sensing_date.month,
                "day": scene.sensing_date.day,
                "mean_value": float(cache.mean_value) if cache and cache.mean_value else None,
                "sensing_date": scene.sensing_date.isoformat()
            }
            for scene, cache in results
        ]
    
    return {
        "entity_id": entity_id,
        "index_type": index_type,
        "current_year": {
            "year": current_year,
            "stats": get_year_stats(current_year)
        },
        "previous_year": {
            "year": current_year - 1,
            "stats": get_year_stats(current_year - 1)
        }
    }
